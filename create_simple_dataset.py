import os
import json
from bs4 import BeautifulSoup
import re

# –¢–û–õ–¨–ö–û –≠–¢–ò –°–ï–ö–¶–ò–ò (–±–µ–∑ additional_info!)
ALLOWED_SECTIONS = {
    "definition": ["–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—è", "–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ"],
    "etiology": ["–≠—Ç–∏–æ–ª–æ–≥–∏—è"],
    "epidemiology": ["–≠–ø–∏–¥–µ–º–∏–æ–ª–æ–≥–∏—è"],
    "clinical_picture": ["–ö–ª–∏–Ω–∏—á–µ—Å–∫–∞—è –∫–∞—Ä—Ç–∏–Ω–∞"],
    "diagnostics": ["–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞", "–ö—Ä–∏—Ç–µ—Ä–∏–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –¥–∏–∞–≥–Ω–æ–∑–∞"],
    "complaints_anamnesis": ["–ñ–∞–ª–æ–±—ã –∏ –∞–Ω–∞–º–Ω–µ–∑", "–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–∏ —Å–±–æ—Ä–µ –∂–∞–ª–æ–±"],
    "physical_examination": ["–§–∏–∑–∏–∫–∞–ª—å–Ω–æ–µ –æ–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ", "–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø–∞—Ü–∏–µ–Ω—Ç–∞–º –ø—Ä–∏ –ø–æ–¥–æ–∑—Ä–µ–Ω–∏–∏"],
    "lab_diagnostics": ["–õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω—ã–µ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è", "–õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞"],
    "instrumental_diagnostics": ["–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è", "–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞"],
    "other_diagnostics": ["–ò–Ω—ã–µ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è", "–î–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–µ –∞–Ω–≥–∏–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥–∏–∫–∏"]
}

def clean_text(text):
    """–£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã –∏ –ø—Ä–æ–±–µ–ª—ã"""
    text = re.sub(r'\n+', '\n', text)  # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–µ—Ä–µ–Ω–æ—Å—ã –≤ –æ–¥–∏–Ω
    text = re.sub(r' +', ' ', text)     # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –≤ –æ–¥–∏–Ω
    return text.strip()

def parse_html_clean(filepath):
    """–ü–∞—Ä—Å–∏–Ω–≥ –¢–û–õ–¨–ö–û —Ä–∞–∑—Ä–µ—à–µ–Ω–Ω—ã—Ö —Å–µ–∫—Ü–∏–π"""
    with open(filepath, encoding='utf-8') as f:
        soup = BeautifulSoup(f, 'html.parser')

    # –ò–∑–≤–ª–µ–∫–∞–µ–º title
    title = soup.title.get_text(strip=True) if soup.title else os.path.basename(filepath)

    # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç
    body_text = soup.body.get_text(separator='\n', strip=True)
    lines = [line.strip() for line in body_text.split('\n') if line.strip()]

    # –ü–∞—Ä—Å–∏–º —Å–µ–∫—Ü–∏–∏ –ø–æ –Ω–æ–º–µ—Ä–∞–º (–Ω–∞–ø—Ä–∏–º–µ—Ä "1.2 –≠—Ç–∏–æ–ª–æ–≥–∏—è")
    section_pattern = re.compile(r"^([0-9]+(?:\.[0-9]+)*\.?\s+.+)")
    sections_raw = {}
    current_section = None
    buffer = []
    
    for line in lines:
        if section_pattern.match(line):
            if current_section:
                sections_raw[current_section] = "\n".join(buffer).strip()
                buffer = []
            current_section = line
        else:
            if current_section:
                buffer.append(line)
                
    if current_section and buffer:
        sections_raw[current_section] = "\n".join(buffer).strip()

    # –§–∏–ª—å—Ç—Ä—É–µ–º –¢–û–õ–¨–ö–û –Ω—É–∂–Ω—ã–µ —Å–µ–∫—Ü–∏–∏
    final_sections = {}
    
    for key, patterns in ALLOWED_SECTIONS.items():
        for section_name, section_text in sections_raw.items():
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å –ª—é–±—ã–º –∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
            if any(pattern.lower() in section_name.lower() for pattern in patterns):
                final_sections[key] = clean_text(section_text)
                break  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ

    # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ë–ï–ó additional_info –∏ images
    output = {
        "title": title,
        "sections": final_sections
    }

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    basename = os.path.splitext(os.path.basename(filepath))[0]
    outdir = os.path.join(os.getcwd(), 'cleaned_dataset')
    os.makedirs(outdir, exist_ok=True)
    outpath = os.path.join(outdir, f'{basename}.json')
    
    with open(outpath, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    
    print(f"‚úÖ {basename}: {len(final_sections)} —Å–µ–∫—Ü–∏–π")
    return outpath

def batch_parse(directory='clin_rec_html'):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ HTML —Ñ–∞–π–ª—ã"""
    if not os.path.exists(directory):
        print(f"‚ùå –ü–∞–ø–∫–∞ {directory} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
        return
    
    files = [f for f in os.listdir(directory) if f.lower().endswith('.html')]
    print(f"üìÅ –ù–∞–π–¥–µ–Ω–æ {len(files)} HTML —Ñ–∞–π–ª–æ–≤\n")
    
    success = 0
    for filename in files:
        filepath = os.path.join(directory, filename)
        try:
            parse_html_clean(filepath)
            success += 1
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ {filename}: {e}")
    
    print(f"\n‚ú® –ì–æ—Ç–æ–≤–æ! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {success}/{len(files)} —Ñ–∞–π–ª–æ–≤")
    print(f"üìÇ –î–∞—Ç–∞—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ 'cleaned_dataset/'")

if __name__ == "__main__":
    batch_parse()

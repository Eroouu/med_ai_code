from pathlib import Path
import json
import tempfile
from datetime import datetime
from langchain_community.vectorstores import FAISS
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
class MedicalInterviewBot:
    def __init__(self, rebuild_db: bool = False):
        self.script_dir = Path(__file__).parent
        self.data_dir = self.script_dir / "cleaned_dataset"
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º temp –ø–∞–ø–∫—É (–≤—Å–µ–≥–¥–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π –ø—É—Ç—å)
        temp_base = Path(tempfile.gettempdir())
        self.db_dir = temp_base / "medical_bot_db"
        
        self.conversation_history = []
        self.collected_info = {
            "chief_complaint": "",
            "symptoms": [],
            "duration": "",
            "additional_info": []
        }
        
        print("=" * 70)
        print("üè• –ú–ï–î–ò–¶–ò–ù–°–ö–ò–ô –ò–ù–¢–ï–†–í–¨–Æ–ï–† v2.5 (Temp DB)")
        print("=" * 70)
        print(f"\nüìÅ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö: {self.db_dir}")
        
        if not self.data_dir.exists():
            print(f"\n‚ùå –ü–∞–ø–∫–∞ {self.data_dir} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç!")
            exit(1)
        
        if rebuild_db and self.db_dir.exists():
            print("\nüóëÔ∏è –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞...")
            import shutil
            shutil.rmtree(self.db_dir)
            print("   ‚úÖ –£–¥–∞–ª—ë–Ω")
        
        self._load_or_create_knowledge_base()
        
        print("\nü§ñ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏...")
        self.llm = ChatOllama(model="llama3.1", temperature=0.3)
        print("   ‚úÖ llama3.1 –≥–æ—Ç–æ–≤–∞")
        
        print("\n" + "=" * 70)
        print("‚úÖ –°–ò–°–¢–ï–ú–ê –ì–û–¢–û–í–ê!")
        print("=" * 70)
    
    def _load_or_create_knowledge_base(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞"""
        
        embeddings = OllamaEmbeddings(model="nomic-embed-text")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∏–Ω–¥–µ–∫—Å
        if self.db_dir.exists() and (self.db_dir / "index.faiss").exists():
            print("\nüìö –ù–∞–π–¥–µ–Ω —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π FAISS –∏–Ω–¥–µ–∫—Å")
            print(f"   –ü—É—Ç—å: {self.db_dir}")
            
            try:
                print("   ‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞...")
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º load_local –≤–º–µ—Å—Ç–æ pickle
                self.vectorstore = FAISS.load_local(
                    str(self.db_dir),
                    embeddings,
                    allow_dangerous_deserialization=True
                )
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞
                test = self.vectorstore.similarity_search("—Ç–µ—Å—Ç", k=1)
                print("   ‚úÖ –ò–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ")
                return
                
            except Exception as e:
                print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞: {e}")
                print("   üîÑ –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π...")
        
        print("\nüìö –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ FAISS –∏–Ω–¥–µ–∫—Å–∞")
        print("   ‚è≥ –ó–∞–π–º—ë—Ç 5-15 –º–∏–Ω—É—Ç\n")
        
        self._create_new_database(embeddings)
    
    def _create_new_database(self, embeddings):
        """–°–æ–∑–¥–∞–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞"""
        
        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        print("1Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
        documents = []
        json_files = list(self.data_dir.glob("*.json"))
        
        if not json_files:
            print("   ‚ùå –ù–µ—Ç JSON —Ñ–∞–π–ª–æ–≤!")
            exit(1)
        
        total = len(json_files)
        print(f"   –ù–∞–π–¥–µ–Ω–æ: {total} —Ñ–∞–π–ª–æ–≤")
        
        for i, json_file in enumerate(json_files, 1):
            try:
                with open(json_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                
                title = data.get("title", "")
                full_text = f"# {title}\n\n"
                
                if "sections" in data:
                    for section_name, section_text in data["sections"].items():
                        if section_text and str(section_text).strip():
                            readable_name = section_name.replace("_", " ").title()
                            full_text += f"## {readable_name}\n{section_text}\n\n"
                
                if len(full_text) > 100:
                    doc = Document(
                        page_content=full_text,
                        metadata={"title": title, "disease": title}
                    )
                    documents.append(doc)
                
                # –ü—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 50 —Ñ–∞–π–ª–æ–≤
                if i % 50 == 0 or i == total:
                    print(f"   üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {i}/{total}")
                    
            except Exception as e:
                print(f"   ‚ö†Ô∏è {json_file.name}: {e}")
        
        print(f"   ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(documents)} –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–π")
        
        # 2. –†–∞–∑–±–∏–≤–∫–∞
        print("\n2Ô∏è‚É£ –†–∞–∑–±–∏–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞...")
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,
            chunk_overlap=100
        )
        splits = text_splitter.split_documents(documents)
        total_splits = len(splits)
        print(f"   ‚úÖ –§—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {total_splits}")
        
        # 3. –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –ø–æ—Ä—Ü–∏—è–º–∏
        print("\n3Ô∏è‚É£ –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
        print("   ‚è≥ –≠—Ç–æ –∑–∞–π–º—ë—Ç –≤—Ä–µ–º—è - –Ω–∞–±–µ—Ä–∏—Ç–µ—Å—å —Ç–µ—Ä–ø–µ–Ω–∏—è\n")
        
        batch_size = 100
        vectorstore = None
        
        try:
            for i in range(0, total_splits, batch_size):
                batch = splits[i:i+batch_size]
                
                if vectorstore is None:
                    # –ü–µ—Ä–≤–∞—è –ø–æ—Ä—Ü–∏—è
                    vectorstore = FAISS.from_documents(batch, embeddings)
                else:
                    # –î–æ–±–∞–≤–ª—è–µ–º –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π
                    vectorstore.add_documents(batch)
                
                # –ü—Ä–æ–≥—Ä–µ—Å—Å
                progress = min(i + batch_size, total_splits)
                percentage = (progress / total_splits) * 100
                print(f"   üìä {progress}/{total_splits} ({percentage:.1f}%)")
            
            self.vectorstore = vectorstore
            
            # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ save_local
            print("\n4Ô∏è‚É£ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞...")
            self.db_dir.mkdir(parents=True, exist_ok=True)  # ‚Üê –ù–û–í–ê–Ø –í–ï–†–°–ò–Ø (–¥–æ–±–∞–≤–∏–ª–∏ parents=True)

            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º save_local –≤–º–µ—Å—Ç–æ pickle - —ç—Ç–æ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Å–ø–æ—Å–æ–±
            self.vectorstore.save_local(str(self.db_dir))
            
            print(f"   ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤: {self.db_dir}")
            
        except Exception as e:
            print(f"\n   ‚ùå –û—à–∏–±–∫–∞: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    def _search_context(self, query: str, k: int = 3) -> str:
        try:
            docs = self.vectorstore.similarity_search(query, k=k)
            context = "\n\n".join([doc.page_content[:700] for doc in docs])
            return context
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return ""
    
    def _generate_question(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–æ–ø—Ä–æ—Å–∞ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        search_query = f"{self.collected_info['chief_complaint']} {' '.join(self.collected_info['symptoms'])}"
        context = self._search_context(search_query, k=2)
        
        history = "\n".join([
            f"{'–í—Ä–∞—á' if m['role'] == 'assistant' else '–ü–∞—Ü–∏–µ–Ω—Ç'}: {m['content']}"
            for m in self.conversation_history[-4:]
        ])
    
        prompt = ChatPromptTemplate.from_template("""
–¢—ã –≤—Ä–∞—á, —Å–æ–±–∏—Ä–∞—é—â–∏–π –∞–Ω–∞–º–Ω–µ–∑.

–ò–°–¢–û–†–ò–Ø:
{history}

–ò–ù–§–û–†–ú–ê–¶–ò–Ø:
- –ñ–∞–ª–æ–±–∞: {chief_complaint}
- –°–∏–º–ø—Ç–æ–º—ã: {symptoms}

–ö–õ–ò–ù–ò–ß–ï–°–ö–ò–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:
{context}

–ó–∞–¥–∞–π –û–î–ò–ù –∫–æ—Ä–æ—Ç–∫–∏–π –≤–æ–ø—Ä–æ—Å –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è —Å–∏–º–ø—Ç–æ–º–æ–≤.

–í–æ–ø—Ä–æ—Å:""")
    
        try:
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º timeout –∏ –¥–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            from langchain_core.runnables import RunnableConfig
            
            response = self.llm.invoke(
                prompt.format(
                    history=history,
                    chief_complaint=self.collected_info["chief_complaint"] or "–Ω–µ —É–∫–∞–∑–∞–Ω–æ",
                    symptoms=", ".join(self.collected_info["symptoms"]) if self.collected_info["symptoms"] else "–Ω–µ—Ç",
                    context=context or "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö"
                ),
                config=RunnableConfig(
                    max_concurrency=1,
                    timeout=30  # 30 —Å–µ–∫—É–Ω–¥ timeout
                )
            )
            return response.content.strip()
        except Exception as e:
            print(f"\n‚ö†Ô∏è –û—à–∏–±–∫–∞ LLM: {e}")
            # Fallback –≤–æ–ø—Ä–æ—Å—ã
            fallback_questions = [
                "–ö–∞–∫ –¥–∞–≤–Ω–æ —É –≤–∞—Å —ç—Ç–∏ —Å–∏–º–ø—Ç–æ–º—ã?",
                "–£—Å–∏–ª–∏–≤–∞—é—Ç—Å—è –ª–∏ —Å–∏–º–ø—Ç–æ–º—ã –ø–æ—Å–ª–µ –µ–¥—ã?",
                "–ï—Å—Ç—å –ª–∏ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞?",
                "–ë—ã–ª–∞ –ª–∏ —Ä–≤–æ—Ç–∞?",
                "–ì–¥–µ –∏–º–µ–Ω–Ω–æ –ª–æ–∫–∞–ª–∏–∑—É–µ—Ç—Å—è –±–æ–ª—å?"
            ]
            import random
            return random.choice(fallback_questions)
    
    def _extract_info(self, text: str):
        text_lower = text.lower()
        
        time_words = ['–¥–µ–Ω—å', '–¥–Ω—è', '–¥–Ω–µ–π', '–Ω–µ–¥–µ–ª—é', '–º–µ—Å—è—Ü', '–≥–æ–¥']
        if any(w in text_lower for w in time_words) and not self.collected_info["duration"]:
            self.collected_info["duration"] = text
        
        symptoms = ['–±–æ–ª—å', '—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞', '—Ç–æ—à–Ω–æ—Ç–∞', '—Ä–≤–æ—Ç–∞', '—Å–ª–∞–±–æ—Å—Ç—å',
                   '–∫–∞—à–µ–ª—å', '–Ω–∞—Å–º–æ—Ä–∫', '–≥–æ—Ä–ª–æ', '–≥–æ–ª–æ–≤–∞', '–∂–∏–≤–æ—Ç']
        
        for symptom in symptoms:
            if symptom in text_lower:
                if symptom not in " ".join(self.collected_info["symptoms"]).lower():
                    self.collected_info["symptoms"].append(symptom)
    
    def _should_continue(self) -> bool:
        questions = len([m for m in self.conversation_history if m["role"] == "assistant"])
        has_info = (
            bool(self.collected_info["chief_complaint"]) and
            (len(self.collected_info["symptoms"]) >= 2 or bool(self.collected_info["duration"]))
        )
        return questions < 8 and not has_info
    
    def _generate_report(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        search_query = " ".join([
            self.collected_info["chief_complaint"],
            *self.collected_info["symptoms"]
        ])
        context = self._search_context(search_query, k=5)
        
        conversation = "\n".join([
            f"{'–í—Ä–∞—á' if m['role'] == 'assistant' else '–ü–∞—Ü–∏–µ–Ω—Ç'}: {m['content']}"
            for m in self.conversation_history
        ])
        
        prompt = ChatPromptTemplate.from_template("""
–°–æ—Å—Ç–∞–≤—å –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –æ—Ç—á—ë—Ç –¥–ª—è –≤—Ä–∞—á–∞.

–ë–ï–°–ï–î–ê:
{conversation}

–ö–õ–ò–ù–ò–ß–ï–°–ö–ò–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:
{context}

–§–æ—Ä–º–∞—Ç:

**Anamnesis morbi:**
[–ò—Å—Ç–æ—Ä–∏—è –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—è]

**Differential diagnosis:**
[–í–æ–∑–º–æ–∂–Ω—ã–µ –¥–∏–∞–≥–Ω–æ–∑—ã]

**Recommendations:**
[–ü–ª–∞–Ω –æ–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è]

–û—Ç—á—ë—Ç:""")
    
        try:
            from langchain_core.runnables import RunnableConfig
            
            print("   ‚è≥ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞ (–º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å 10-30 —Å–µ–∫—É–Ω–¥)...")
            
            response = self.llm.invoke(
                prompt.format(
                    conversation=conversation,
                    context=context or "–¢—Ä–µ–±—É–µ—Ç—Å—è –æ–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ"
                ),
                config=RunnableConfig(
                    timeout=60  # 60 —Å–µ–∫—É–Ω–¥ –¥–ª—è –æ—Ç—á—ë—Ç–∞
                )
            )
            return response.content
        except Exception as e:
            print(f"\n‚ö†Ô∏è –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            
            # Fallback - –ø—Ä–æ—Å—Ç–æ–π –æ—Ç—á—ë—Ç
            return f"""**Anamnesis morbi:**
–ü–∞—Ü–∏–µ–Ω—Ç –æ–±—Ä–∞—Ç–∏–ª—Å—è —Å –∂–∞–ª–æ–±–∞–º–∏: {self.collected_info['chief_complaint']}
–°–∏–º–ø—Ç–æ–º—ã: {', '.join(self.collected_info['symptoms']) if self.collected_info['symptoms'] else '–Ω–µ —É–∫–∞–∑–∞–Ω—ã'}
–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {self.collected_info['duration'] if self.collected_info['duration'] else '–Ω–µ —É–∫–∞–∑–∞–Ω–∞'}

**Differential diagnosis:**
–¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–ª—è –ø–æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –¥–∏–∞–≥–Ω–æ–∑–∞.

**Recommendations:**
- –ö–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è –≤—Ä–∞—á–∞
- –û–±—â–∏–π –∞–Ω–∞–ª–∏–∑ –∫—Ä–æ–≤–∏
- –£–ó–ò –æ—Ä–≥–∞–Ω–æ–≤ –±—Ä—é—à–Ω–æ–π –ø–æ–ª–æ—Å—Ç–∏
- –ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ - –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è"""
    
    def start_interview(self):
        print("\n" + "=" * 70)
        print("ü©∫ –ú–ï–î–ò–¶–ò–ù–°–ö–û–ï –ò–ù–¢–ï–†–í–¨–Æ")
        print("=" * 70)
        print("\n–ö–æ–º–∞–Ω–¥—ã: '—Å—Ç–æ–ø' - –∑–∞–≤–µ—Ä—à–∏—Ç—å, 'exit' - –≤—ã—Ö–æ–¥\n")
        
        greeting = "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –ß—Ç–æ –≤–∞—Å –±–µ—Å–ø–æ–∫–æ–∏—Ç?"
        print(f"ü§ñ: {greeting}\n")
        self.conversation_history.append({"role": "assistant", "content": greeting})
        
        complaint = input("üë§: ").strip()
        
        if complaint.lower() in ['exit', '–≤—ã—Ö–æ–¥']:
            print("\nüëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            return
        
        if not complaint:
            print("‚ö†Ô∏è –í–≤–µ–¥–∏—Ç–µ –∂–∞–ª–æ–±—É")
            return
        
        self.collected_info["chief_complaint"] = complaint
        self.conversation_history.append({"role": "user", "content": complaint})
        self._extract_info(complaint)
        
        while self._should_continue():
            try:
                question = self._generate_question()
                print(f"\nü§ñ: {question}\n")
                self.conversation_history.append({"role": "assistant", "content": question})
                
                answer = input("üë§: ").strip()
                
                if answer.lower() in ['exit', '–≤—ã—Ö–æ–¥']:
                    print("\nüëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                    return
                
                if answer.lower() == '—Å—Ç–æ–ø':
                    break
                
                if not answer:
                    continue
                
                self.conversation_history.append({"role": "user", "content": answer})
                self._extract_info(answer)
                
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞: {e}")
                break
        
        print("\n" + "=" * 70)
        print("üìã –ì–ï–ù–ï–†–ê–¶–ò–Ø –û–¢–ß–Å–¢–ê...")
        print("=" * 70)
        
        try:
            report = self._generate_report()
            
            print("\n" + "=" * 70)
            print("üìÑ –ú–ï–î–ò–¶–ò–ù–°–ö–ò–ô –û–¢–ß–Å–¢")
            print("=" * 70 + "\n")
            print(report)
            print("\n" + "=" * 70)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_file = self.script_dir / f"report_{timestamp}.txt"
            
            with open(report_file, "w", encoding="utf-8") as f:
                f.write(f"–ú–ï–î–ò–¶–ò–ù–°–ö–ò–ô –û–¢–ß–Å–¢\n")
                f.write(f"–î–∞—Ç–∞: {datetime.now().strftime('%d.%m.%Y %H:%M')}\n")
                f.write("=" * 70 + "\n\n")
                f.write(report)
            
            print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {report_file.name}")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    import sys
    rebuild = "--rebuild" in sys.argv
    
    try:
        bot = MedicalInterviewBot(rebuild_db=rebuild)
        bot.start_interview()
    except KeyboardInterrupt:
        print("\n\nüëã –ü—Ä–µ—Ä–≤–∞–Ω–æ")
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()

from pathlib import Path
import json
import pickle
from datetime import datetime
from langchain_community.vectorstores import FAISS
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate

class MedicalInterviewBot:
    def __init__(self, rebuild_db: bool = False):
        self.script_dir = Path(__file__).parent
        self.data_dir = self.script_dir / "cleaned_dataset"
        self.db_file = self.script_dir / "faiss_index.pkl"
        
        self.conversation_history = []
        self.collected_info = {
            "chief_complaint": "",
            "symptoms": [],
            "duration": "",
            "additional_info": []
        }
        
        print("=" * 70)
        print("üè• –ú–ï–î–ò–¶–ò–ù–°–ö–ò–ô –ò–ù–¢–ï–†–í–¨–Æ–ï–† v2.1 (FAISS)")
        print("=" * 70)
        
        if not self.data_dir.exists():
            print(f"\n‚ùå –ü–∞–ø–∫–∞ {self.data_dir} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç!")
            exit(1)
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–π –∏–Ω–¥–µ–∫—Å –µ—Å–ª–∏ rebuild
        if rebuild_db and self.db_file.exists():
            print("\nüóëÔ∏è –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞...")
            self.db_file.unlink()
            print("   ‚úÖ –£–¥–∞–ª—ë–Ω")
        
        self._load_or_create_knowledge_base()
        
        print("\nü§ñ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏...")
        self.llm = ChatOllama(model="llama3.1", temperature=0.3)
        print("   ‚úÖ llama3.1 –≥–æ—Ç–æ–≤–∞")
        
        print("\n" + "=" * 70)
        print("‚úÖ –°–ò–°–¢–ï–ú–ê –ì–û–¢–û–í–ê!")
        print("=" * 70)
    
    def _load_or_create_knowledge_base(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞"""
        
        embeddings = OllamaEmbeddings(model="nomic-embed-text")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∏–Ω–¥–µ–∫—Å
        if self.db_file.exists():
            print("\nüìö –ù–∞–π–¥–µ–Ω —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π FAISS –∏–Ω–¥–µ–∫—Å")
            print(f"   –ü—É—Ç—å: {self.db_file}")
            
            try:
                with open(self.db_file, "rb") as f:
                    self.vectorstore = pickle.load(f)
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏
                test = self.vectorstore.similarity_search("—Ç–µ—Å—Ç", k=1)
                
                print("   ‚úÖ –ò–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ")
                return
                
            except Exception as e:
                print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
                print("   üîÑ –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å...")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
        print("\nüìö –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ FAISS –∏–Ω–¥–µ–∫—Å–∞")
        print("   ‚è≥ –ó–∞–π–º—ë—Ç 2-5 –º–∏–Ω—É—Ç\n")
        
        self._create_new_database(embeddings)
    
    def _create_new_database(self, embeddings):
        """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ FAISS –∏–Ω–¥–µ–∫—Å–∞"""
        
        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        print("1Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
        documents = []
        json_files = list(self.data_dir.glob("*.json"))
        
        if not json_files:
            print("   ‚ùå –ù–µ—Ç JSON —Ñ–∞–π–ª–æ–≤!")
            exit(1)
        
        print(f"   –ù–∞–π–¥–µ–Ω–æ: {len(json_files)} —Ñ–∞–π–ª–æ–≤")
        
        for i, json_file in enumerate(json_files, 1):
            try:
                with open(json_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                
                title = data.get("title", "")
                full_text = f"# {title}\n\n"
                
                if "sections" in data:
                    for section_name, section_text in data["sections"].items():
                        if section_text and str(section_text).strip():
                            readable_name = section_name.replace("_", " ").title()
                            full_text += f"## {readable_name}\n{section_text}\n\n"
                
                if len(full_text) > 100:
                    doc = Document(
                        page_content=full_text,
                        metadata={"title": title, "disease": title}
                    )
                    documents.append(doc)
                    
                if i % 50 == 0:
                    print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {i}/{len(json_files)}")
                    
            except Exception as e:
                print(f"   ‚ö†Ô∏è {json_file.name}: {e}")
        
        print(f"   ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(documents)} –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–π")
        
        # 2. –†–∞–∑–±–∏–≤–∫–∞
        print("\n2Ô∏è‚É£ –†–∞–∑–±–∏–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞...")
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=150
        )
        splits = text_splitter.split_documents(documents)
        print(f"   ‚úÖ –§—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {len(splits)}")
        
        # 3. –°–æ–∑–¥–∞–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞
        print("\n3Ô∏è‚É£ –°–æ–∑–¥–∞–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞...")
        print("   ‚è≥ –ü–æ–¥–æ–∂–¥–∏—Ç–µ...")
        
        try:
            # FAISS —Å–æ–∑–¥–∞—ë—Ç—Å—è –∑–∞ –æ–¥–∏–Ω —Ä–∞–∑ - –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ
            self.vectorstore = FAISS.from_documents(splits, embeddings)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å
            print("\n4Ô∏è‚É£ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞...")
            with open(self.db_file, "wb") as f:
                pickle.dump(self.vectorstore, f)
            
            print(f"   ‚úÖ –ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {self.db_file}")
            
        except Exception as e:
            print(f"\n   ‚ùå –û—à–∏–±–∫–∞: {e}")
            raise
    
    def _search_context(self, query: str, k: int = 3) -> str:
        """–ü–æ–∏—Å–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        try:
            docs = self.vectorstore.similarity_search(query, k=k)
            context = "\n\n".join([doc.page_content[:700] for doc in docs])
            return context
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return ""
    
    def _generate_question(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–æ–ø—Ä–æ—Å–∞"""
        search_query = f"{self.collected_info['chief_complaint']} {' '.join(self.collected_info['symptoms'])}"
        context = self._search_context(search_query, k=2)
        
        history = "\n".join([
            f"{'–í—Ä–∞—á' if m['role'] == 'assistant' else '–ü–∞—Ü–∏–µ–Ω—Ç'}: {m['content']}"
            for m in self.conversation_history[-4:]
        ])
        
        prompt = ChatPromptTemplate.from_template("""
–¢—ã –≤—Ä–∞—á, —Å–æ–±–∏—Ä–∞—é—â–∏–π –∞–Ω–∞–º–Ω–µ–∑.

–ò–°–¢–û–†–ò–Ø:
{history}

–ò–ù–§–û–†–ú–ê–¶–ò–Ø:
- –ñ–∞–ª–æ–±–∞: {chief_complaint}
- –°–∏–º–ø—Ç–æ–º—ã: {symptoms}

–ö–õ–ò–ù–ò–ß–ï–°–ö–ò–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:
{context}

–ó–∞–¥–∞–π –û–î–ò–ù –∫–æ—Ä–æ—Ç–∫–∏–π –≤–æ–ø—Ä–æ—Å –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è —Å–∏–º–ø—Ç–æ–º–æ–≤.

–í–æ–ø—Ä–æ—Å:""")
        
        try:
            response = self.llm.invoke(prompt.format(
                history=history,
                chief_complaint=self.collected_info["chief_complaint"] or "–Ω–µ —É–∫–∞–∑–∞–Ω–æ",
                symptoms=", ".join(self.collected_info["symptoms"]) if self.collected_info["symptoms"] else "–Ω–µ—Ç",
                context=context or "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö"
            ))
            return response.content.strip()
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞: {e}")
            return "–†–∞—Å—Å–∫–∞–∂–∏—Ç–µ –ø–æ–¥—Ä–æ–±–Ω–µ–µ?"
    
    def _extract_info(self, text: str):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
        text_lower = text.lower()
        
        time_words = ['–¥–µ–Ω—å', '–¥–Ω—è', '–¥–Ω–µ–π', '–Ω–µ–¥–µ–ª—é', '–º–µ—Å—è—Ü', '–≥–æ–¥']
        if any(w in text_lower for w in time_words) and not self.collected_info["duration"]:
            self.collected_info["duration"] = text
        
        symptoms = ['–±–æ–ª—å', '—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞', '—Ç–æ—à–Ω–æ—Ç–∞', '—Ä–≤–æ—Ç–∞', '—Å–ª–∞–±–æ—Å—Ç—å',
                   '–∫–∞—à–µ–ª—å', '–Ω–∞—Å–º–æ—Ä–∫', '–≥–æ—Ä–ª–æ', '–≥–æ–ª–æ–≤–∞', '–∂–∏–≤–æ—Ç']
        
        for symptom in symptoms:
            if symptom in text_lower:
                if symptom not in " ".join(self.collected_info["symptoms"]).lower():
                    self.collected_info["symptoms"].append(symptom)
    
    def _should_continue(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è"""
        questions = len([m for m in self.conversation_history if m["role"] == "assistant"])
        has_info = (
            bool(self.collected_info["chief_complaint"]) and
            (len(self.collected_info["symptoms"]) >= 2 or bool(self.collected_info["duration"]))
        )
        return questions < 8 and not has_info
    
    def _generate_report(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞"""
        search_query = " ".join([
            self.collected_info["chief_complaint"],
            *self.collected_info["symptoms"]
        ])
        context = self._search_context(search_query, k=5)
        
        conversation = "\n".join([
            f"{'–í—Ä–∞—á' if m['role'] == 'assistant' else '–ü–∞—Ü–∏–µ–Ω—Ç'}: {m['content']}"
            for m in self.conversation_history
        ])
        
        prompt = ChatPromptTemplate.from_template("""
–°–æ—Å—Ç–∞–≤—å –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –æ—Ç—á—ë—Ç –¥–ª—è –≤—Ä–∞—á–∞.

–ë–ï–°–ï–î–ê:
{conversation}

–ö–õ–ò–ù–ò–ß–ï–°–ö–ò–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:
{context}

–§–æ—Ä–º–∞—Ç:

**Anamnesis morbi:**
[–ò—Å—Ç–æ—Ä–∏—è –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—è]

**Differential diagnosis:**
[–í–æ–∑–º–æ–∂–Ω—ã–µ –¥–∏–∞–≥–Ω–æ–∑—ã]

**Recommendations:**
[–ü–ª–∞–Ω –æ–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è]

–û—Ç—á—ë—Ç:""")
        
        try:
            response = self.llm.invoke(prompt.format(
                conversation=conversation,
                context=context or "–¢—Ä–µ–±—É–µ—Ç—Å—è –æ–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ"
            ))
            return response.content
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞: {e}")
            return "–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"
    
    def start_interview(self):
        """–ó–∞–ø—É—Å–∫ –∏–Ω—Ç–µ—Ä–≤—å—é"""
        print("\n" + "=" * 70)
        print("ü©∫ –ú–ï–î–ò–¶–ò–ù–°–ö–û–ï –ò–ù–¢–ï–†–í–¨–Æ")
        print("=" * 70)
        print("\n–ö–æ–º–∞–Ω–¥—ã: '—Å—Ç–æ–ø' - –∑–∞–≤–µ—Ä—à–∏—Ç—å, 'exit' - –≤—ã—Ö–æ–¥\n")
        
        greeting = "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –ß—Ç–æ –≤–∞—Å –±–µ—Å–ø–æ–∫–æ–∏—Ç?"
        print(f"ü§ñ: {greeting}\n")
        self.conversation_history.append({"role": "assistant", "content": greeting})
        
        complaint = input("üë§: ").strip()
        
        if complaint.lower() in ['exit', '–≤—ã—Ö–æ–¥']:
            print("\nüëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            return
        
        if not complaint:
            print("‚ö†Ô∏è –í–≤–µ–¥–∏—Ç–µ –∂–∞–ª–æ–±—É")
            return
        
        self.collected_info["chief_complaint"] = complaint
        self.conversation_history.append({"role": "user", "content": complaint})
        self._extract_info(complaint)
        
        while self._should_continue():
            try:
                question = self._generate_question()
                print(f"\nü§ñ: {question}\n")
                self.conversation_history.append({"role": "assistant", "content": question})
                
                answer = input("üë§: ").strip()
                
                if answer.lower() in ['exit', '–≤—ã—Ö–æ–¥']:
                    print("\nüëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                    return
                
                if answer.lower() == '—Å—Ç–æ–ø':
                    break
                
                if not answer:
                    continue
                
                self.conversation_history.append({"role": "user", "content": answer})
                self._extract_info(answer)
                
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞: {e}")
                break
        
        print("\n" + "=" * 70)
        print("üìã –ì–ï–ù–ï–†–ê–¶–ò–Ø –û–¢–ß–Å–¢–ê...")
        print("=" * 70)
        
        try:
            report = self._generate_report()
            
            print("\n" + "=" * 70)
            print("üìÑ –ú–ï–î–ò–¶–ò–ù–°–ö–ò–ô –û–¢–ß–Å–¢")
            print("=" * 70 + "\n")
            print(report)
            print("\n" + "=" * 70)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_file = self.script_dir / f"report_{timestamp}.txt"
            
            with open(report_file, "w", encoding="utf-8") as f:
                f.write(f"–ú–ï–î–ò–¶–ò–ù–°–ö–ò–ô –û–¢–ß–Å–¢\n")
                f.write(f"–î–∞—Ç–∞: {datetime.now().strftime('%d.%m.%Y %H:%M')}\n")
                f.write("=" * 70 + "\n\n")
                f.write(report)
            
            print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {report_file.name}")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    import sys
    rebuild = "--rebuild" in sys.argv
    
    try:
        bot = MedicalInterviewBot(rebuild_db=rebuild)
        bot.start_interview()
    except KeyboardInterrupt:
        print("\n\nüëã –ü—Ä–µ—Ä–≤–∞–Ω–æ")
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()

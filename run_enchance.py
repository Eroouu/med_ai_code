from pathlib import Path
import json
import tempfile
from datetime import datetime

from langchain_community.vectorstores import FAISS
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate


class MedicalInterviewBot:
    def __init__(self, rebuild_db: bool = False):
        self.script_dir = Path(__file__).parent
        # –ù–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç–∞—Å–µ—Ç–∞
        self.data_dir = self.script_dir / "enhanced_dataset"

        # –í—Ä–µ–º–µ–Ω–Ω–∞—è –ø–∞–ø–∫–∞ –¥–ª—è FAISS‚Äë–∏–Ω–¥–µ–∫—Å–∞
        temp_base = Path(tempfile.gettempdir())
        self.db_dir = temp_base / "medical_bot_db"

        self.conversation_history = []
        self.collected_info = {
            "chief_complaint": "",
            "symptoms": [],
            "duration": "",
            "additional_info": []
        }

        print("=" * 70)
        print("üè• –ú–ï–î–ò–¶–ò–ù–°–ö–ò–ô –ò–ù–¢–ï–†–í–¨–Æ–ï–† v2.6 (enhanced_dataset)")
        print("=" * 70)
        print(f"\nüìÅ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö: {self.db_dir}")

        if not self.data_dir.exists():
            print(f"\n‚ùå –ü–∞–ø–∫–∞ {self.data_dir} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç!")
            exit(1)

        if rebuild_db and self.db_dir.exists():
            print("\nüóëÔ∏è –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞...")
            import shutil
            shutil.rmtree(self.db_dir)
            print(" ‚úÖ –£–¥–∞–ª—ë–Ω")

        self._load_or_create_knowledge_base()

        print("\nü§ñ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏...")
        self.llm = ChatOllama(model="llama3.1", temperature=0.3)
        print(" ‚úÖ llama3.1 –≥–æ—Ç–æ–≤–∞")

        print("\n" + "=" * 70)
        print("‚úÖ –°–ò–°–¢–ï–ú–ê –ì–û–¢–û–í–ê!")
        print("=" * 70)

    # ---------- –†–∞–±–æ—Ç–∞ —Å –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π ----------

    def _load_or_create_knowledge_base(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞ –ø–æ–¥ enhanced_dataset."""
        embeddings = OllamaEmbeddings(model="nomic-embed-text")

        if self.db_dir.exists() and (self.db_dir / "index.faiss").exists():
            print("\nüìö –ù–∞–π–¥–µ–Ω —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π FAISS –∏–Ω–¥–µ–∫—Å")
            print(f" –ü—É—Ç—å: {self.db_dir}")
            try:
                print(" ‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞...")
                self.vectorstore = FAISS.load_local(
                    str(self.db_dir),
                    embeddings,
                    allow_dangerous_deserialization=True,
                )
                # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
                _ = self.vectorstore.similarity_search("—Ç–µ—Å—Ç", k=1)
                print(" ‚úÖ –ò–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ")
                return
            except Exception as e:
                print(f" ‚ö†Ô∏è –û—à–∏–±–∫–∞: {e}")
                print(" üîÑ –ü–µ—Ä–µ—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å...")

        print("\nüìö –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ FAISS –∏–Ω–¥–µ–∫—Å–∞ (enhanced_dataset)")
        print(" ‚è≥ –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å 5‚Äì15 –º–∏–Ω—É—Ç\n")
        self._create_new_database(embeddings)

    def _create_new_database(self, embeddings):
        """–°–æ–∑–¥–∞–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞ –∏–∑ enhanced_dataset."""
        print("1Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")

        documents = []
        json_files = list(self.data_dir.glob("*.json"))

        if not json_files:
            print(" ‚ùå –í –ø–∞–ø–∫–µ enhanced_dataset –Ω–µ—Ç JSON‚Äë—Ñ–∞–π–ª–æ–≤!")
            exit(1)

        total = len(json_files)
        print(f" –ù–∞–π–¥–µ–Ω–æ: {total} —Ñ–∞–π–ª–æ–≤")

        for i, json_file in enumerate(json_files, 1):
            try:
                with open(json_file, "r", encoding="utf-8") as f:
                    data = json.load(f)

                title = (data.get("title") or "").strip()

                # –û—Å–Ω–æ–≤–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã
                sections = data.get("sections", {})
                full_text = f"# {title}\n\n"

                for section_name, section_text in sections.items():
                    if not section_text or not str(section_text).strip():
                        continue
                    readable_name = section_name.replace("_", " ").title()
                    full_text += f"## {readable_name}\n{section_text}\n\n"

                if len(full_text) <= 100:
                    # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –¥–æ–∫—É–º–µ–Ω—Ç ‚Äì –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                    continue

                # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ enhance_medical_dataset
                meta = data.get("metadata", {}) or {}
                doc_metadata = {
                    "title": title,
                    "disease": title,
                    "file": json_file.name,
                    "categories": meta.get("categories", []),
                    "symptoms": meta.get("symptoms", []),
                    "complexity": meta.get("complexity", ""),
                    "symptoms_count": meta.get("symptoms_count", 0),
                    "filled_sections": meta.get("filled_sections", 0),
                    "total_sections": meta.get("total_sections", 0),
                    "completeness_score": meta.get("completeness_score", 0.0),
                }

                documents.append(
                    Document(page_content=full_text, metadata=doc_metadata)
                )

                if i % 50 == 0 or i == total:
                    print(f" üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {i}/{total}")

            except Exception as e:
                print(f" ‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ {json_file.name}: {e}")

        print(f" ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–π: {len(documents)}")

        # 2. –†–∞–∑–±–∏–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞
        print("\n2Ô∏è‚É£ –†–∞–∑–±–∏–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã...")
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,
            chunk_overlap=100,
        )
        splits = text_splitter.split_documents(documents)
        total_splits = len(splits)
        print(f" ‚úÖ –§—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {total_splits}")

        # 3. –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        print("\n3Ô∏è‚É£ –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
        print(" ‚è≥ –≠—Ç–æ –∑–∞–π–º—ë—Ç –≤—Ä–µ–º—è, –¥–æ–∂–¥–∏—Ç–µ—Å—å –æ–∫–æ–Ω—á–∞–Ω–∏—è\n")

        batch_size = 100
        vectorstore = None

        try:
            for i in range(0, total_splits, batch_size):
                batch = splits[i : i + batch_size]
                if vectorstore is None:
                    vectorstore = FAISS.from_documents(batch, embeddings)
                else:
                    vectorstore.add_documents(batch)

                progress = min(i + batch_size, total_splits)
                percent = (progress / total_splits) * 100
                print(f" üìä {progress}/{total_splits} ({percent:.1f}%)")

            self.vectorstore = vectorstore

            # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
            print("\n4Ô∏è‚É£ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞...")
            self.db_dir.mkdir(parents=True, exist_ok=True)
            self.vectorstore.save_local(str(self.db_dir))
            print(f" ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤: {self.db_dir}")

        except Exception as e:
            print(f"\n ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∏–Ω–¥–µ–∫—Å–∞: {e}")
            import traceback

            traceback.print_exc()
            raise

    def _search_context(self, query: str, k: int = 3) -> str:
        """–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ FAISS."""
        try:
            docs = self.vectorstore.similarity_search(query, k=k)
            return "\n\n".join(doc.page_content[:700] for doc in docs)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return ""

    # ---------- –õ–æ–≥–∏–∫–∞ –¥–∏–∞–ª–æ–≥–∞ ----------

    def _generate_question(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ –≤—Ä–∞—á—É‚Äë–±–æ—Ç–æ–º."""
        search_query = f"{self.collected_info['chief_complaint']} " \
                       f"{' '.join(self.collected_info['symptoms'])}"
        context = self._search_context(search_query, k=2)

        history = "\n".join(
            f"{'–í—Ä–∞—á' if m['role'] == 'assistant' else '–ü–∞—Ü–∏–µ–Ω—Ç'}: {m['content']}"
            for m in self.conversation_history[-4:]
        )

        prompt = ChatPromptTemplate.from_template(
            """
–¢—ã –≤—Ä–∞—á, —Å–æ–±–∏—Ä–∞—é—â–∏–π –∞–Ω–∞–º–Ω–µ–∑. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ —É–∑–Ω–∞—Ç—å –∫–∞–∫ –º–æ–∂–Ω–æ –±–æ–ª—å—à–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–∞—Ü–∏–µ–Ω—Ç–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –µ–≥–æ –∑–¥–æ—Ä–æ–≤—å—è,
–¥–ª—è —Ç–æ–≥–æ —á—Ç–æ–±—ã –∑–∞–ø–æ–ª–Ω–∏—Ç—å. –í –Ω–µ–≥–æ –≤—Ö–æ–¥–∏—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—É–Ω–∫—Ç–æ–≤: –û–±—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∑–¥–æ—Ä–æ–≤—å—è –ø–∞—Ü–∏–µ–Ω—Ç–∞, –∫–∞–∫–∏–µ —É –Ω–µ–≥–æ —Å–∏–º–ø—Ç–æ–º—ã, –∫–∞–∫–∏–µ –ª–µ–∫–∞—Ä—Å—Ç–≤–∞ –æ–Ω –ø—Ä–∏–Ω–µ–º–∞–µ—Ç —Å–µ–π—á–∞
–∏ –∫–∞–∫ –ø—Ä–∏–Ω–∏–º–∞–ª –≤ –±–ª–∏–∂–∞–π—â–µ–µ –≤—Ä–µ–º—è –¥–æ —ç—Ç–æ–≥–æ.

–ò–°–¢–û–†–ò–Ø:
{history}

–ò–ù–§–û–†–ú–ê–¶–ò–Ø:
- –ñ–∞–ª–æ–±–∞: {chief_complaint}
- –°–∏–º–ø—Ç–æ–º—ã: {symptoms}

–ö–õ–ò–ù–ò–ß–ï–°–ö–ò–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:
{context}

–ó–∞–¥–∞–π –û–î–ò–ù –∫–æ—Ä–æ—Ç–∫–∏–π —É—Ç–æ—á–Ω—è—é—â–∏–π –≤–æ–ø—Ä–æ—Å.
–í–æ–ø—Ä–æ—Å:"""
        )

        try:
            from langchain_core.runnables import RunnableConfig

            response = self.llm.invoke(
                prompt.format(
                    history=history,
                    chief_complaint=self.collected_info["chief_complaint"]
                    or "–Ω–µ —É–∫–∞–∑–∞–Ω–æ",
                    symptoms=", ".join(self.collected_info["symptoms"])
                    if self.collected_info["symptoms"]
                    else "–Ω–µ—Ç",
                    context=context or "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö",
                ),
                config=RunnableConfig(max_concurrency=1, timeout=30),
            )
            return response.content.strip()
        except Exception as e:
            print(f"\n‚ö†Ô∏è –û—à–∏–±–∫–∞ LLM: {e}")
            fallback_questions = [
                "–ö–∞–∫ –¥–∞–≤–Ω–æ —É –≤–∞—Å —ç—Ç–∏ —Å–∏–º–ø—Ç–æ–º—ã?",
                "–£—Å–∏–ª–∏–≤–∞—é—Ç—Å—è –ª–∏ —Å–∏–º–ø—Ç–æ–º—ã –ø–æ—Å–ª–µ –µ–¥—ã?",
                "–ï—Å—Ç—å –ª–∏ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞?",
                "–ë—ã–ª–∞ –ª–∏ —Ä–≤–æ—Ç–∞?",
                "–ì–¥–µ –∏–º–µ–Ω–Ω–æ –ª–æ–∫–∞–ª–∏–∑—É–µ—Ç—Å—è –±–æ–ª—å?",
            ]
            import random

            return random.choice(fallback_questions)

    def _extract_info(self, text: str):
        """–ì—Ä—É–±–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–∏–º–ø—Ç–æ–º–æ–≤ –∏ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–∑ –æ—Ç–≤–µ—Ç–∞ –ø–∞—Ü–∏–µ–Ω—Ç–∞."""
        text_lower = text.lower()

        time_words = ["–¥–µ–Ω—å", "–¥–Ω—è", "–¥–Ω–µ–π", "–Ω–µ–¥–µ–ª—é", "–º–µ—Å—è—Ü", "–≥–æ–¥"]
        if any(w in text_lower for w in time_words) and not self.collected_info[
            "duration"
        ]:
            self.collected_info["duration"] = text

        symptoms_vocab = [
            "–±–æ–ª—å",
            "—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞",
            "—Ç–æ—à–Ω–æ—Ç–∞",
            "—Ä–≤–æ—Ç–∞",
            "—Å–ª–∞–±–æ—Å—Ç—å",
            "–∫–∞—à–µ–ª—å",
            "–Ω–∞—Å–º–æ—Ä–∫",
            "–≥–æ—Ä–ª–æ",
            "–≥–æ–ª–æ–≤–∞",
            "–∂–∏–≤–æ—Ç",
        ]
        for symptom in symptoms_vocab:
            if symptom in text_lower:
                if symptom not in " ".join(
                    self.collected_info["symptoms"]
                ).lower():
                    self.collected_info["symptoms"].append(symptom)

    def _should_continue(self) -> bool:
        """–†–µ—à–µ–Ω–∏–µ, –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å –ª–∏ –∏–Ω—Ç–µ—Ä–≤—å—é."""
        questions = len(
            [m for m in self.conversation_history if m["role"] == "assistant"]
        )
        
        # –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ —É—Å–ª–æ–≤–∏—è –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏:
        # 1. –î–æ–ª–∂–Ω–æ –±—ã—Ç—å –∂–∞–ª–æ–±–∞
        # 2. –ú–∏–Ω–∏–º—É–º 3-4 —Å–∏–º–ø—Ç–æ–º–∞ –ò –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        has_enough_info = (
            bool(self.collected_info["chief_complaint"]) and
            len(self.collected_info["symptoms"]) >= 3 and  # –Ω–µ 2, –∞ 3+
            bool(self.collected_info["duration"])  # –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        )
        
        # –ú–∞–∫—Å–∏–º—É–º 15 –≤–æ–ø—Ä–æ—Å–æ–≤, –Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º, –µ—Å–ª–∏ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        return questions < 15 and not has_enough_info


    def _generate_report(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç–æ–≥–æ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–≥–æ –æ—Ç—á—ë—Ç–∞ —Å –∞–Ω–∞–º–Ω–µ–∑–æ–º."""
        
        search_query = " ".join([
            self.collected_info["chief_complaint"],
            *self.collected_info["symptoms"]
        ])
        context = self._search_context(search_query, k=5)
        
        conversation = "\n".join([
            f"{'–í—Ä–∞—á' if m['role'] == 'assistant' else '–ü–∞—Ü–∏–µ–Ω—Ç'}: {m['content']}"
            for m in self.conversation_history
        ])
        
        prompt = ChatPromptTemplate.from_template("""
–¢—ã –æ–ø—ã—Ç–Ω—ã–π –≤—Ä–∞—á, –≥–æ—Ç–æ–≤—è—â–∏–π –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–º–Ω–µ–∑ –ø–∞—Ü–∏–µ–Ω—Ç–∞ –¥–ª—è –∫–æ–ª–ª–µ–≥.
–ù–∞ –æ—Å–Ω–æ–≤–µ –¥–∏–∞–ª–æ–≥–∞ –Ω–∏–∂–µ, –∑–∞–ø–æ–ª–Ω–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –æ—Ç—á—ë—Ç.

–î–ò–ê–õ–û–ì –° –ü–ê–¶–ò–ï–ù–¢–û–ú:
{conversation}

–ö–õ–ò–ù–ò–ß–ï–°–ö–ò–ï –î–ê–ù–ù–´–ï –ò–ó –ë–ê–ó–´:
{context}

–°–û–ë–†–ê–ù–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø:
- –û—Å–Ω–æ–≤–Ω–∞—è –∂–∞–ª–æ–±–∞: {chief_complaint}
- –°–∏–º–ø—Ç–æ–º—ã: {symptoms}
- –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {duration}
- –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ: {additional_info}

–ó–∞–ø–æ–ª–Ω–∏ –ø–æ–¥—Ä–æ–±–Ω—ã–π –°–¢–†–£–ö–¢–£–†–ò–†–û–í–ê–ù–ù–´–ô –ê–ù–ê–ú–ù–ï–ó –¥–ª—è –≤—Ä–∞—á–∞:

**ANAMNESIS VITAE (–ò—Å—Ç–æ—Ä–∏—è –∂–∏–∑–Ω–∏):**
[–ó–∞–ø–æ–ª–Ω–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞: –≤–æ–∑—Ä–∞—Å—Ç, –ø–æ–ª (–µ—Å–ª–∏ —É–ø–æ–º—è–Ω—É—Ç), –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ –≤—Ä–µ–¥–Ω–æ—Å—Ç–∏, 
–æ–±—Ä–∞–∑ –∂–∏–∑–Ω–∏, –∫—É—Ä–µ–Ω–∏–µ/–∞–ª–∫–æ–≥–æ–ª—å, —Ö—Ä–æ–Ω–∏—á–µ—Å–∫–∏–µ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—è, –∞–ª–µ—Ä–≥–∏–∏]

**ANAMNESIS MORBI (–ò—Å—Ç–æ—Ä–∏—è –±–æ–ª–µ–∑–Ω–∏):**
[–†–∞–∑–≤–µ—Ä–Ω—É—Ç–æ –æ–ø–∏—à–∏: –Ω–∞—á–∞–ª–æ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—è, —Ç–µ—á–µ–Ω–∏–µ, —Ä–∞–∑–≤–∏—Ç–∏–µ —Å–∏–º–ø—Ç–æ–º–æ–≤, —Ñ–∞–∫—Ç–æ—Ä—ã, —É—Å–∏–ª–∏–≤–∞—é—â–∏–µ/–æ—Å–ª–∞–±–ª—è—é—â–∏–µ]

**–ñ–ê–õ–û–ë–´ –ò –°–ò–ú–ü–¢–û–ú–´:**
[–ü–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–ø—Ç–æ–º–∞: —Ö–∞—Ä–∞–∫—Ç–µ—Ä, –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å, –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è, –≤—Ä–µ–º—è –ø–æ—è–≤–ª–µ–Ω–∏—è]

**–î–ò–§–§–ï–†–ï–ù–¶–ò–ê–õ–¨–ù–´–ô –î–ò–ê–ì–ù–û–ó:**
[–ù–∞ –æ—Å–Ω–æ–≤–µ –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏ —Å–∏–º–ø—Ç–æ–º–æ–≤ –≤—ã–¥–≤–∏–Ω–∏ 3-5 –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã—Ö –¥–∏–∞–≥–Ω–æ–∑–æ–≤ —Å –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ–º]

**–ü–õ–ê–ù –û–ë–°–õ–ï–î–û–í–ê–ù–ò–Ø:**
[–ü–µ—Ä–µ—á–∏—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∞–Ω–∞–ª–∏–∑—ã –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è, —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤]

**–ü–†–ò–ú–ï–ß–ê–ù–ò–Ø –î–õ–Ø –í–†–ê–ß–ê:**
[–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ –æ–±—Ä–∞—Ç–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ –ø—Ä–∏ –æ—Å–º–æ—Ç—Ä–µ]

–ì–æ—Ç–æ–≤—ã–π –æ—Ç—á—ë—Ç:""")
    
        try:
            from langchain_core.runnables import RunnableConfig
            
            print(" ‚è≥ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–¥—Ä–æ–±–Ω–æ–≥–æ –æ—Ç—á—ë—Ç–∞ (30-60 —Å–µ–∫—É–Ω–¥)...")
            response = self.llm.invoke(
                prompt.format(
                    conversation=conversation,
                    context=context or "–¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ",
                    chief_complaint=self.collected_info["chief_complaint"] or "–Ω–µ —É–∫–∞–∑–∞–Ω–∞",
                    symptoms=", ".join(self.collected_info["symptoms"]) if self.collected_info["symptoms"] else "–Ω–µ —É–∫–∞–∑–∞–Ω—ã",
                    duration=self.collected_info["duration"] or "–Ω–µ —É–∫–∞–∑–∞–Ω–∞",
                    additional_info=", ".join(self.collected_info["additional_info"]) if self.collected_info["additional_info"] else "–æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç"
                ),
                config=RunnableConfig(
                    timeout=90  # –ë–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á—ë—Ç
                ),
            )
            return response.content
            
        except Exception as e:
            print(f"\n‚ö†Ô∏è –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            
            # Fallback - —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç—á—ë—Ç –≤—Ä—É—á–Ω—É—é
            return f"""
**ANAMNESIS VITAE:**
–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∏—Å—Ç–æ—Ä–∏–∏ –∂–∏–∑–Ω–∏ –Ω–µ –±—ã–ª–∞ —Å–æ–±—Ä–∞–Ω–∞ –≤ —Ö–æ–¥–µ –ø–µ—Ä–≤–∏—á–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤—å—é.

**ANAMNESIS MORBI:**
–ü–∞—Ü–∏–µ–Ω—Ç –æ–±—Ä–∞—Ç–∏–ª—Å—è —Å –æ—Å–Ω–æ–≤–Ω–æ–π –∂–∞–ª–æ–±–æ–π –Ω–∞: {self.collected_info['chief_complaint']}

–ù–∞—á–∞–ª–æ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—è: {self.collected_info['duration'] if self.collected_info['duration'] else '–≤—Ä–µ–º—è –Ω–∞—á–∞–ª–∞ –Ω–µ —É—Ç–æ—á–Ω–µ–Ω–æ'}

**–ñ–ê–õ–û–ë–´ –ò –°–ò–ú–ü–¢–û–ú–´:**
{chr(10).join(f"- {s.capitalize()}" for s in self.collected_info['symptoms']) if self.collected_info['symptoms'] else "- –°–∏–º–ø—Ç–æ–º—ã –Ω–µ —É–∫–∞–∑–∞–Ω—ã"}

**–î–ò–§–§–ï–†–ï–ù–¶–ò–ê–õ–¨–ù–´–ô –î–ò–ê–ì–ù–û–ó:**
–ù–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥—ä—è–≤–ª–µ–Ω–Ω—ã—Ö –∂–∞–ª–æ–± –∏ —Å–∏–º–ø—Ç–æ–º–æ–≤ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å:
- –û—Å—Ç—Ä—ã–µ –∏–Ω—Ñ–µ–∫—Ü–∏–æ–Ω–Ω—ã–µ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—è
- –•—Ä–æ–Ω–∏—á–µ—Å–∫–∏–µ —Å–∏—Å—Ç–µ–º–Ω—ã–µ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—è
- –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Ä–∞—Å—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
- –ü—Å–∏—Ö–æ—Å–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –Ω–∞—Ä—É—à–µ–Ω–∏—è

–¢—Ä–µ–±—É–µ—Ç—Å—è —É—Ç–æ—á–Ω—è—é—â–µ–µ –æ–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ.

**–ü–õ–ê–ù –û–ë–°–õ–ï–î–û–í–ê–ù–ò–Ø:**
1. –û–±—â–∏–π –∞–Ω–∞–ª–∏–∑ –∫—Ä–æ–≤–∏ (–û–ê–ö)
2. –û–±—â–∏–π –∞–Ω–∞–ª–∏–∑ –º–æ—á–∏ (–û–ê–ú)
3. –ë–∏–æ—Ö–∏–º–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∫—Ä–æ–≤–∏
4. –£–ó–ò –æ—Ä–≥–∞–Ω–æ–≤ –±—Ä—é—à–Ω–æ–π –ø–æ–ª–æ—Å—Ç–∏ (–ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –∂–∞–ª–æ–± –Ω–∞ –±–æ–ª–∏ –≤ –∂–∏–≤–æ—Ç–µ)
5. –≠–ö–ì (–ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –∂–∞–ª–æ–± –Ω–∞ –±–æ–ª–∏ –≤ –≥—Ä—É–¥–∏ –∏–ª–∏ –Ω–∞—Ä—É—à–µ–Ω–∏–µ —Ä–∏—Ç–º–∞)
6. –ü–æ –ø–æ–∫–∞–∑–∞–Ω–∏—è–º - –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤

**–ü–†–ò–ú–ï–ß–ê–ù–ò–Ø –î–õ–Ø –í–†–ê–ß–ê:**
- –¢—Ä–µ–±—É–µ—Ç—Å—è –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω–æ–µ —É—Ç–æ—á–Ω–µ–Ω–∏–µ –∞–Ω–∞–º–Ω–µ–∑–∞
- –ù–µ–æ–±—Ö–æ–¥–∏–º–∞ —Ñ–∏–∑–∏–∫–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
- –ù–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–æ–≤ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π –¥–∏–∞–≥–Ω–æ–∑
"""

    # ---------- –ó–∞–ø—É—Å–∫ –∏–Ω—Ç–µ—Ä–≤—å—é ----------

    def start_interview(self):
        print("\n" + "=" * 70)
        print("ü©∫ –ú–ï–î–ò–¶–ò–ù–°–ö–û–ï –ò–ù–¢–ï–†–í–¨–Æ")
        print("=" * 70)
        print("\n–ö–æ–º–∞–Ω–¥—ã: '—Å—Ç–æ–ø' ‚Äî –∑–∞–≤–µ—Ä—à–∏—Ç—å, 'exit' ‚Äî –≤—ã—Ö–æ–¥\n")

        greeting = "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –ß—Ç–æ –≤–∞—Å –±–µ—Å–ø–æ–∫–æ–∏—Ç?"
        print(f"ü§ñ: {greeting}\n")
        self.conversation_history.append({"role": "assistant", "content": greeting})

        complaint = input("üë§: ").strip()
        if complaint.lower() in ["exit", "–≤—ã—Ö–æ–¥"]:
            print("\nüëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            return
        if not complaint:
            print("‚ö†Ô∏è –í–≤–µ–¥–∏—Ç–µ –∂–∞–ª–æ–±—É")
            return

        self.collected_info["chief_complaint"] = complaint
        self.conversation_history.append({"role": "user", "content": complaint})
        self._extract_info(complaint)

        while self._should_continue():
            try:
                question = self._generate_question()
                print(f"\nü§ñ: {question}\n")
                self.conversation_history.append(
                    {"role": "assistant", "content": question}
                )

                answer = input("üë§: ").strip()
                if answer.lower() in ["exit", "–≤—ã—Ö–æ–¥"]:
                    print("\nüëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                    return
                if answer.lower() == "—Å—Ç–æ–ø":
                    break
                if not answer:
                    continue

                self.conversation_history.append(
                    {"role": "user", "content": answer}
                )
                self._extract_info(answer)
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞: {e}")
                break

        print("\n" + "=" * 70)
        print("üìã –ì–ï–ù–ï–†–ê–¶–ò–Ø –û–¢–ß–Å–¢–ê...")
        print("=" * 70)

        try:
            report = self._generate_report()
            print("\n" + "=" * 70)
            print("üìÑ –ú–ï–î–ò–¶–ò–ù–°–ö–ò–ô –û–¢–ß–Å–¢")
            print("=" * 70 + "\n")
            print(report)
            print("\n" + "=" * 70)

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_file = self.script_dir / f"report_{timestamp}.txt"
            with open(report_file, "w", encoding="utf-8") as f:
                f.write("–ú–ï–î–ò–¶–ò–ù–°–ö–ò–ô –û–¢–ß–Å–¢\n")
                f.write(
                    f"–î–∞—Ç–∞: {datetime.now().strftime('%d.%m.%Y %H:%M')}\n"
                )
                f.write("=" * 70 + "\n\n")
                f.write(report)

            print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ —Ñ–∞–π–ª: {report_file.name}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –æ—Ç—á—ë—Ç–∞: {e}")


if __name__ == "__main__":
    import sys

    rebuild = "--rebuild" in sys.argv
    try:
        bot = MedicalInterviewBot(rebuild_db=rebuild)
        bot.start_interview()
    except KeyboardInterrupt:
        print("\n\nüëã –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback

        traceback.print_exc()

from pathlib import Path
import json
import tempfile
from datetime import datetime

from langchain_community.vectorstores import FAISS
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate


class MedicalInterviewBot:
    def __init__(self, rebuild_db: bool = False):
        self.script_dir = Path(__file__).parent
        # –ù–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç–∞—Å–µ—Ç–∞
        self.data_dir = self.script_dir / "enhanced_dataset"

        # –í—Ä–µ–º–µ–Ω–Ω–∞—è –ø–∞–ø–∫–∞ –¥–ª—è FAISS‚Äë–∏–Ω–¥–µ–∫—Å–∞
        temp_base = Path(tempfile.gettempdir())
        self.db_dir = temp_base / "medical_bot_db"

        self.conversation_history = []
        self.collected_info = {
            "chief_complaint": "",
            "symptoms": [],
            "duration": "",
            "additional_info": []
        }

        print("=" * 70)
        print("üè• –ú–ï–î–ò–¶–ò–ù–°–ö–ò–ô –ò–ù–¢–ï–†–í–¨–Æ–ï–† v2.6 (enhanced_dataset)")
        print("=" * 70)
        print(f"\nüìÅ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö: {self.db_dir}")

        if not self.data_dir.exists():
            print(f"\n‚ùå –ü–∞–ø–∫–∞ {self.data_dir} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç!")
            exit(1)

        if rebuild_db and self.db_dir.exists():
            print("\nüóëÔ∏è –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞...")
            import shutil
            shutil.rmtree(self.db_dir)
            print(" ‚úÖ –£–¥–∞–ª—ë–Ω")

        self._load_or_create_knowledge_base()

        print("\nü§ñ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏...")
        self.llm = ChatOllama(model="llama3.1", temperature=0.3)
        print(" ‚úÖ llama3.1 –≥–æ—Ç–æ–≤–∞")

        print("\n" + "=" * 70)
        print("‚úÖ –°–ò–°–¢–ï–ú–ê –ì–û–¢–û–í–ê!")
        print("=" * 70)

    # ---------- –†–∞–±–æ—Ç–∞ —Å –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π ----------

    def _load_or_create_knowledge_base(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞ –ø–æ–¥ enhanced_dataset."""
        embeddings = OllamaEmbeddings(model="nomic-embed-text")

        if self.db_dir.exists() and (self.db_dir / "index.faiss").exists():
            print("\nüìö –ù–∞–π–¥–µ–Ω —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π FAISS –∏–Ω–¥–µ–∫—Å")
            print(f" –ü—É—Ç—å: {self.db_dir}")
            try:
                print(" ‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞...")
                self.vectorstore = FAISS.load_local(
                    str(self.db_dir),
                    embeddings,
                    allow_dangerous_deserialization=True,
                )
                # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
                _ = self.vectorstore.similarity_search("—Ç–µ—Å—Ç", k=1)
                print(" ‚úÖ –ò–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ")
                return
            except Exception as e:
                print(f" ‚ö†Ô∏è –û—à–∏–±–∫–∞: {e}")
                print(" üîÑ –ü–µ—Ä–µ—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å...")

        print("\nüìö –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ FAISS –∏–Ω–¥–µ–∫—Å–∞ (enhanced_dataset)")
        print(" ‚è≥ –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å 5‚Äì15 –º–∏–Ω—É—Ç\n")
        self._create_new_database(embeddings)

    def _create_new_database(self, embeddings):
        """–°–æ–∑–¥–∞–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞ –∏–∑ enhanced_dataset."""
        print("1Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")

        documents = []
        json_files = list(self.data_dir.glob("*.json"))

        if not json_files:
            print(" ‚ùå –í –ø–∞–ø–∫–µ enhanced_dataset –Ω–µ—Ç JSON‚Äë—Ñ–∞–π–ª–æ–≤!")
            exit(1)

        total = len(json_files)
        print(f" –ù–∞–π–¥–µ–Ω–æ: {total} —Ñ–∞–π–ª–æ–≤")

        for i, json_file in enumerate(json_files, 1):
            try:
                with open(json_file, "r", encoding="utf-8") as f:
                    data = json.load(f)

                title = (data.get("title") or "").strip()

                # –û—Å–Ω–æ–≤–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã
                sections = data.get("sections", {})
                full_text = f"# {title}\n\n"

                for section_name, section_text in sections.items():
                    if not section_text or not str(section_text).strip():
                        continue
                    readable_name = section_name.replace("_", " ").title()
                    full_text += f"## {readable_name}\n{section_text}\n\n"

                if len(full_text) <= 100:
                    # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –¥–æ–∫—É–º–µ–Ω—Ç ‚Äì –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                    continue

                # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ enhance_medical_dataset
                meta = data.get("metadata", {}) or {}
                doc_metadata = {
                    "title": title,
                    "disease": title,
                    "file": json_file.name,
                    "categories": meta.get("categories", []),
                    "symptoms": meta.get("symptoms", []),
                    "complexity": meta.get("complexity", ""),
                    "symptoms_count": meta.get("symptoms_count", 0),
                    "filled_sections": meta.get("filled_sections", 0),
                    "total_sections": meta.get("total_sections", 0),
                    "completeness_score": meta.get("completeness_score", 0.0),
                }

                documents.append(
                    Document(page_content=full_text, metadata=doc_metadata)
                )

                if i % 50 == 0 or i == total:
                    print(f" üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {i}/{total}")

            except Exception as e:
                print(f" ‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ {json_file.name}: {e}")

        print(f" ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–π: {len(documents)}")

        # 2. –†–∞–∑–±–∏–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞
        print("\n2Ô∏è‚É£ –†–∞–∑–±–∏–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã...")
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,
            chunk_overlap=100,
        )
        splits = text_splitter.split_documents(documents)
        total_splits = len(splits)
        print(f" ‚úÖ –§—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {total_splits}")

        # 3. –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        print("\n3Ô∏è‚É£ –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
        print(" ‚è≥ –≠—Ç–æ –∑–∞–π–º—ë—Ç –≤—Ä–µ–º—è, –¥–æ–∂–¥–∏—Ç–µ—Å—å –æ–∫–æ–Ω—á–∞–Ω–∏—è\n")

        batch_size = 100
        vectorstore = None

        try:
            for i in range(0, total_splits, batch_size):
                batch = splits[i : i + batch_size]
                if vectorstore is None:
                    vectorstore = FAISS.from_documents(batch, embeddings)
                else:
                    vectorstore.add_documents(batch)

                progress = min(i + batch_size, total_splits)
                percent = (progress / total_splits) * 100
                print(f" üìä {progress}/{total_splits} ({percent:.1f}%)")

            self.vectorstore = vectorstore

            # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
            print("\n4Ô∏è‚É£ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞...")
            self.db_dir.mkdir(parents=True, exist_ok=True)
            self.vectorstore.save_local(str(self.db_dir))
            print(f" ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤: {self.db_dir}")

        except Exception as e:
            print(f"\n ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∏–Ω–¥–µ–∫—Å–∞: {e}")
            import traceback

            traceback.print_exc()
            raise

    def _search_context(self, query: str, k: int = 3) -> str:
        """–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ FAISS."""
        try:
            docs = self.vectorstore.similarity_search(query, k=k)
            return "\n\n".join(doc.page_content[:700] for doc in docs)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return ""

    # ---------- –õ–æ–≥–∏–∫–∞ –¥–∏–∞–ª–æ–≥–∞ ----------

    def _generate_question(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ –≤—Ä–∞—á—É‚Äë–±–æ—Ç–æ–º."""
        search_query = f"{self.collected_info['chief_complaint']} " \
                       f"{' '.join(self.collected_info['symptoms'])}"
        context = self._search_context(search_query, k=2)

        history = "\n".join(
            f"{'–í—Ä–∞—á' if m['role'] == 'assistant' else '–ü–∞—Ü–∏–µ–Ω—Ç'}: {m['content']}"
            for m in self.conversation_history[-4:]
        )

        prompt = ChatPromptTemplate.from_template(
            """
–¢—ã –≤—Ä–∞—á, —Å–æ–±–∏—Ä–∞—é—â–∏–π –∞–Ω–∞–º–Ω–µ–∑.

–ò–°–¢–û–†–ò–Ø:
{history}

–ò–ù–§–û–†–ú–ê–¶–ò–Ø:
- –ñ–∞–ª–æ–±–∞: {chief_complaint}
- –°–∏–º–ø—Ç–æ–º—ã: {symptoms}

–ö–õ–ò–ù–ò–ß–ï–°–ö–ò–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:
{context}

–ó–∞–¥–∞–π –û–î–ò–ù –∫–æ—Ä–æ—Ç–∫–∏–π —É—Ç–æ—á–Ω—è—é—â–∏–π –≤–æ–ø—Ä–æ—Å.
–í–æ–ø—Ä–æ—Å:"""
        )

        try:
            from langchain_core.runnables import RunnableConfig

            response = self.llm.invoke(
                prompt.format(
                    history=history,
                    chief_complaint=self.collected_info["chief_complaint"]
                    or "–Ω–µ —É–∫–∞–∑–∞–Ω–æ",
                    symptoms=", ".join(self.collected_info["symptoms"])
                    if self.collected_info["symptoms"]
                    else "–Ω–µ—Ç",
                    context=context or "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö",
                ),
                config=RunnableConfig(max_concurrency=1, timeout=30),
            )
            return response.content.strip()
        except Exception as e:
            print(f"\n‚ö†Ô∏è –û—à–∏–±–∫–∞ LLM: {e}")
            fallback_questions = [
                "–ö–∞–∫ –¥–∞–≤–Ω–æ —É –≤–∞—Å —ç—Ç–∏ —Å–∏–º–ø—Ç–æ–º—ã?",
                "–£—Å–∏–ª–∏–≤–∞—é—Ç—Å—è –ª–∏ —Å–∏–º–ø—Ç–æ–º—ã –ø–æ—Å–ª–µ –µ–¥—ã?",
                "–ï—Å—Ç—å –ª–∏ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞?",
                "–ë—ã–ª–∞ –ª–∏ —Ä–≤–æ—Ç–∞?",
                "–ì–¥–µ –∏–º–µ–Ω–Ω–æ –ª–æ–∫–∞–ª–∏–∑—É–µ—Ç—Å—è –±–æ–ª—å?",
            ]
            import random

            return random.choice(fallback_questions)

    def _extract_info(self, text: str):
        """–ì—Ä—É–±–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–∏–º–ø—Ç–æ–º–æ–≤ –∏ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–∑ –æ—Ç–≤–µ—Ç–∞ –ø–∞—Ü–∏–µ–Ω—Ç–∞."""
        text_lower = text.lower()

        time_words = ["–¥–µ–Ω—å", "–¥–Ω—è", "–¥–Ω–µ–π", "–Ω–µ–¥–µ–ª—é", "–º–µ—Å—è—Ü", "–≥–æ–¥"]
        if any(w in text_lower for w in time_words) and not self.collected_info[
            "duration"
        ]:
            self.collected_info["duration"] = text

        symptoms_vocab = [
            "–±–æ–ª—å",
            "—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞",
            "—Ç–æ—à–Ω–æ—Ç–∞",
            "—Ä–≤–æ—Ç–∞",
            "—Å–ª–∞–±–æ—Å—Ç—å",
            "–∫–∞—à–µ–ª—å",
            "–Ω–∞—Å–º–æ—Ä–∫",
            "–≥–æ—Ä–ª–æ",
            "–≥–æ–ª–æ–≤–∞",
            "–∂–∏–≤–æ—Ç",
        ]
        for symptom in symptoms_vocab:
            if symptom in text_lower:
                if symptom not in " ".join(
                    self.collected_info["symptoms"]
                ).lower():
                    self.collected_info["symptoms"].append(symptom)

    def _should_continue(self) -> bool:
        """–†–µ—à–µ–Ω–∏–µ, –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å –ª–∏ –∏–Ω—Ç–µ—Ä–≤—å—é."""
        questions = len(
            [m for m in self.conversation_history if m["role"] == "assistant"]
        )
        has_enough_info = bool(self.collected_info["chief_complaint"]) and (
            len(self.collected_info["symptoms"]) >= 2
            or bool(self.collected_info["duration"])
        )
        return questions < 8 and not has_enough_info

    def _generate_report(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏—Ç–æ–≥–æ–≤–æ–≥–æ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–≥–æ –æ—Ç—á—ë—Ç–∞."""
        search_query = " ".join(
            [self.collected_info["chief_complaint"], *self.collected_info["symptoms"]]
        )
        context = self._search_context(search_query, k=5)

        conversation = "\n".join(
            f"{'–í—Ä–∞—á' if m['role'] == 'assistant' else '–ü–∞—Ü–∏–µ–Ω—Ç'}: {m['content']}"
            for m in self.conversation_history
        )

        prompt = ChatPromptTemplate.from_template(
            """
–°–æ—Å—Ç–∞–≤—å –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –æ—Ç—á—ë—Ç –¥–ª—è –≤—Ä–∞—á–∞.

–ë–ï–°–ï–î–ê:
{conversation}

–ö–õ–ò–ù–ò–ß–ï–°–ö–ò–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:
{context}

–§–æ—Ä–º–∞—Ç:
**Anamnesis morbi:**
[–ò—Å—Ç–æ—Ä–∏—è –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—è]

**Differential diagnosis:**
[–í–æ–∑–º–æ–∂–Ω—ã–µ –¥–∏–∞–≥–Ω–æ–∑—ã]

**Recommendations:**
[–ü–ª–∞–Ω –æ–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è]

–û—Ç—á—ë—Ç:"""
        )

        try:
            from langchain_core.runnables import RunnableConfig

            print(" ‚è≥ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞ (10‚Äì30 —Å–µ–∫—É–Ω–¥)...")
            response = self.llm.invoke(
                prompt.format(
                    conversation=conversation,
                    context=context or "–¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ",
                ),
                config=RunnableConfig(timeout=60),
            )
            return response.content
        except Exception as e:
            print(f"\n‚ö†Ô∏è –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            return f"""**Anamnesis morbi:**
–ü–∞—Ü–∏–µ–Ω—Ç –æ–±—Ä–∞—Ç–∏–ª—Å—è —Å –∂–∞–ª–æ–±–∞–º–∏: {self.collected_info['chief_complaint']}
–°–∏–º–ø—Ç–æ–º—ã: {', '.join(self.collected_info['symptoms']) if self.collected_info['symptoms'] else '–Ω–µ —É–∫–∞–∑–∞–Ω—ã'}
–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {self.collected_info['duration'] if self.collected_info['duration'] else '–Ω–µ —É–∫–∞–∑–∞–Ω–∞'}

**Differential diagnosis:**
–¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–ª—è –ø–æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –¥–∏–∞–≥–Ω–æ–∑–∞.

**Recommendations:**
- –ö–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è –≤—Ä–∞—á–∞
- –û–±—â–∏–π –∞–Ω–∞–ª–∏–∑ –∫—Ä–æ–≤–∏
- –£–ó–ò –æ—Ä–≥–∞–Ω–æ–≤ –±—Ä—é—à–Ω–æ–π –ø–æ–ª–æ—Å—Ç–∏
- –ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ ‚Äî –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è"""

    # ---------- –ó–∞–ø—É—Å–∫ –∏–Ω—Ç–µ—Ä–≤—å—é ----------

    def start_interview(self):
        print("\n" + "=" * 70)
        print("ü©∫ –ú–ï–î–ò–¶–ò–ù–°–ö–û–ï –ò–ù–¢–ï–†–í–¨–Æ")
        print("=" * 70)
        print("\n–ö–æ–º–∞–Ω–¥—ã: '—Å—Ç–æ–ø' ‚Äî –∑–∞–≤–µ—Ä—à–∏—Ç—å, 'exit' ‚Äî –≤—ã—Ö–æ–¥\n")

        greeting = "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –ß—Ç–æ –≤–∞—Å –±–µ—Å–ø–æ–∫–æ–∏—Ç?"
        print(f"ü§ñ: {greeting}\n")
        self.conversation_history.append({"role": "assistant", "content": greeting})

        complaint = input("üë§: ").strip()
        if complaint.lower() in ["exit", "–≤—ã—Ö–æ–¥"]:
            print("\nüëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            return
        if not complaint:
            print("‚ö†Ô∏è –í–≤–µ–¥–∏—Ç–µ –∂–∞–ª–æ–±—É")
            return

        self.collected_info["chief_complaint"] = complaint
        self.conversation_history.append({"role": "user", "content": complaint})
        self._extract_info(complaint)

        while self._should_continue():
            try:
                question = self._generate_question()
                print(f"\nü§ñ: {question}\n")
                self.conversation_history.append(
                    {"role": "assistant", "content": question}
                )

                answer = input("üë§: ").strip()
                if answer.lower() in ["exit", "–≤—ã—Ö–æ–¥"]:
                    print("\nüëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                    return
                if answer.lower() == "—Å—Ç–æ–ø":
                    break
                if not answer:
                    continue

                self.conversation_history.append(
                    {"role": "user", "content": answer}
                )
                self._extract_info(answer)
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞: {e}")
                break

        print("\n" + "=" * 70)
        print("üìã –ì–ï–ù–ï–†–ê–¶–ò–Ø –û–¢–ß–Å–¢–ê...")
        print("=" * 70)

        try:
            report = self._generate_report()
            print("\n" + "=" * 70)
            print("üìÑ –ú–ï–î–ò–¶–ò–ù–°–ö–ò–ô –û–¢–ß–Å–¢")
            print("=" * 70 + "\n")
            print(report)
            print("\n" + "=" * 70)

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_file = self.script_dir / f"report_{timestamp}.txt"
            with open(report_file, "w", encoding="utf-8") as f:
                f.write("–ú–ï–î–ò–¶–ò–ù–°–ö–ò–ô –û–¢–ß–Å–¢\n")
                f.write(
                    f"–î–∞—Ç–∞: {datetime.now().strftime('%d.%m.%Y %H:%M')}\n"
                )
                f.write("=" * 70 + "\n\n")
                f.write(report)

            print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ —Ñ–∞–π–ª: {report_file.name}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –æ—Ç—á—ë—Ç–∞: {e}")


if __name__ == "__main__":
    import sys

    rebuild = "--rebuild" in sys.argv
    try:
        bot = MedicalInterviewBot(rebuild_db=rebuild)
        bot.start_interview()
    except KeyboardInterrupt:
        print("\n\nüëã –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback

        traceback.print_exc()
